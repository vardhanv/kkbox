{"cells":[{"cell_type":"markdown","source":["<h5> \n<ol>\n <li> Identify the features we want to use\n <li> Train the features on multiple models\n <li> Print the results\n</ol> \n</h5>"],"metadata":{}},{"cell_type":"code","source":["\n#uncomment for jupyter notebook\n#import findspark\n#findspark.init()\n\n#import pyspark\n#sc = pyspark.SparkContext()\n#spark = pyspark.sql.SparkSession(sc)\n\n# General imports\nfrom datetime import datetime\nfrom pyspark.sql import SQLContext\nfrom pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\n\n\n\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import MinMaxScaler\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.classification import LinearSVC\n\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n\ns = SQLContext(sc)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["<h5> Convert user logs into monthly data </h5>\n<ol> \n  <li> convert data to year & month </li>\n  <li> add a colum for months before feb 2017 (which is when we need to train for) </li>\n  <li> save as a new table to be used later </li>\n  <li> Uncomment to rerun- commented to prevent the table from being overwritten </li>\n</ol>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["def renameCols(df,oldName, newNames):\n    nDF = df\n    assert(len(df.schema.names) == len(newNames))\n    assert(len(oldName) == len(newNames))\n    for i in range(0,len(df.schema.names)):\n        nDF = nDF.withColumnRenamed(oldName[i],newNames[i])\n    return nDF"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#d = s.sql('select * from unmgmt_user_logs_all_table')\n#r = d.rdd.map(lambda l: (l[0],l[1],l[2],l[3],l[4],l[5],l[6],l[7],l[8],l[1].year,l[1].month,  (2017-l[1].year)*12 - l[1].month + 1) )\n#usrLogWithYM = renameCols(r.toDF(), d.schema.names + ['year','month','months_b4_Feb2017'])\n#usrLogWithYM.show(1)\n#usrGroup = usrLogWithYM.groupby(['msno','year','month']).agg({ 'num_25': 'sum' , 'num_50': 'sum' ,'num_75': 'sum' ,'num_985': 'sum' ,'num_100': 'sum' ,'num_unq': 'sum','total_secs': 'sum','months_b4_Feb2017' : 'avg'})\n#colNames = ','.join(usrGroup.schema.names).replace('(','_').replace(')','').split(',')\n#renamed = renameCols(usrGroup, colNames)\n#renamed.write.saveAsTable('usrGroupByMonthAndYear')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["<h5> Read Transactions data (aggregated by month) </h5>"],"metadata":{}},{"cell_type":"code","source":["transData = s.sql('select * from unmgmt_transactions_table')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["<h7> Add a field for number of months before feb 2017. Feb 2017 = 0, Jan 2017 = 1 </h7>"],"metadata":{}},{"cell_type":"code","source":["nTransData = transData.withColumn('expiring', (2017-year(transData.membership_expire_date))*12 - month(transData.membership_expire_date) + 2).drop('transaction_date').drop('membership_expire_date')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["<h7> Find all feb/2017 expiring accounts </h7>"],"metadata":{}},{"cell_type":"code","source":["tData = nTransData.filter(nTransData.expiring == 0)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["<h5> Read User Data (aggregated by month) </h5>"],"metadata":{}},{"cell_type":"code","source":["usrAvg = s.sql('select * from usrGroupAvgByMonthAndYear')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["usrAvgF = usrAvg.filter(usrAvg.avg_months_b4_Feb2017 == 0).drop('year').drop('month').drop('avg_months_b4_Feb2017')"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["usrAvgF.show(2)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["<h5> Join all the data sets to create a master data set </h5>"],"metadata":{}},{"cell_type":"code","source":["fData = usrAvgF.join(tData,'msno','inner')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["trainDF = s.sql(\"select * from unmgmt_train_table\").join(fData,'msno','inner')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["<h5> Extract, scale and one-hot encode the features from the master data set </h5>"],"metadata":{}},{"cell_type":"code","source":["indexer1 = StringIndexer(inputCol=\"plan_list_price\", outputCol=\"plan_list_price_enc_\", handleInvalid = 'skip')\noneHot1  = OneHotEncoder(inputCol=\"plan_list_price_enc_\", outputCol= \"plan_list_price_\")\n\nindexer2 = StringIndexer(inputCol=\"payment_method_id\", outputCol=\"payment_method_id_enc_\",handleInvalid = 'skip')\noneHot2  = OneHotEncoder(inputCol=\"payment_method_id_enc_\", outputCol= \"payment_method_id_\")\n\nindexer3 = StringIndexer(inputCol=\"payment_plan_days\", outputCol=\"payment_plan_days_enc_\",handleInvalid = 'skip')\noneHot3  = OneHotEncoder(inputCol=\"payment_plan_days_enc_\", outputCol= \"payment_plan_days_\")\n\nfeatureColumns = ['avg_num_100','avg_num_25','avg_num_75','avg_num_unq','avg_num_50','avg_num_985','avg_total_secs','is_auto_renew','is_cancel','plan_list_price_','payment_method_id_','payment_plan_days_']\nassembler = VectorAssembler ( inputCols = featureColumns, outputCol = 'rawFeatures')\nscalar    = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"outOfRangefeatures\", withStd=True, withMean=True)\nminMax    = MinMaxScaler(inputCol=\"outOfRangefeatures\", outputCol=\"features\")\nlr        = LogisticRegression(labelCol = 'is_churn', maxIter = 100,regParam = 0.4)\ngbt       = GBTClassifier(labelCol='is_churn', maxIter=10, maxBins=34)\nsvm       = LinearSVC(maxIter=20,regParam=0.1, labelCol='is_churn')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["np = Pipeline(stages = [indexer1, indexer2,indexer3,oneHot1,oneHot2,oneHot3,assembler,scalar,minMax])\nfeatureDF = np.fit(trainDF).transform(trainDF).select('features','is_churn')"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["<h5> Lets split the churn data into a training and a test </h5>"],"metadata":{}},{"cell_type":"code","source":["(trainChurn, testChurn) = featureDF.randomSplit([0.7,0.3])"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["<h5> Train and evaluate the SVM Model on this data </h5>"],"metadata":{}},{"cell_type":"code","source":["def evaluateModel(test,train,description):\n   predictionAndLabelTest = test.select(['prediction','is_churn']).rdd.map(lambda x: ( float(x[0]) , float(x[1]) ) )\n   predictionAndLabelTrain = train.select(['prediction','is_churn']).rdd.map(lambda x: ( float(x[0]) , float(x[1]) ) )\n\n   metricsTest = BinaryClassificationMetrics(predictionAndLabelTest)\n   metricsTrain = BinaryClassificationMetrics(predictionAndLabelTrain)\n   print \"\\n------------- \" + description + \" -----------------\"\n   print \"Area under precision: test = %s, train = \"  % metricsTest.areaUnderPR, metricsTrain.areaUnderPR\n   print \"Area under       ROC: test = %s\" % metricsTest.areaUnderROC, metricsTrain.areaUnderROC  "],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["model = svm.fit(trainChurn)\npredictionsTest = model.transform(testChurn)\npredictionsTrain = model.transform(trainChurn)\n\nevaluateModel(predictionsTest,predictionsTrain, \"Support Vector Machine\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["<h5> Train and evaluate a GBT Model on this data </h5>"],"metadata":{}},{"cell_type":"code","source":["model = gbt.fit(trainChurn)\npredictionsTest = model.transform(testChurn)\npredictionsTrain = model.transform(trainChurn)\n\nevaluateModel(predictionsTest,predictionsTrain, \"Gradient Boosted Trees\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["<h5> Train and evaluate a Logistic Regression Model on this data </h5>"],"metadata":{}},{"cell_type":"code","source":["model = lr.fit(trainChurn)\npredictionsTest = model.transform(testChurn)\npredictionsTrain = model.transform(trainChurn)\n\nevaluateModel(predictionsTest,predictionsTrain, \"Logistic Regression\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"kkbox-feature-identification","notebookId":2151111393305407},"nbformat":4,"nbformat_minor":0}
